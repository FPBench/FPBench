\documentclass[main.tex]{subfiles}
\begin{document}

\section{Introduction}
\label{sec:intro}

The increasingly urgent demand for reliable software has led to tremendous
advances in automatic program analysis and verification~\cite{cbmc,astree,klee,coverity,mars-code}.
However, these techniques have typically focused on integer programs, and do
not apply to the floating-point computations we depend on for
safety-critical control in avionics or medical devices, nor the analyses
carried out by scientific and computer-aided design applications.  In these
contexts, floating-point accuracy is critical since subtle rounding errors
can lead to significant discrepancies between floating-point results and
the real results developers expect.  Indeed, floating-point
arithmetic is notoriously unintuitive and its sensitivity to roundoff
errors makes such computations fiendishly difficult to debug.
Traditionally, such errors have been addressed by numerical methods experts
who manually analyze and rewrite floating-point code to ensure accuracy and
stability.  However, these manual techniques are difficult to apply and
typically do not lead to independently checkable certificates guaranteeing
the accuracy.

The research community has responded to these challenges by developing
a rich universe of automated techniques that provide guaranteed bounds on the
accuracy of floating-point computations or attempt to automatically improve
accuracy.  For example, Fluctuat~\cite{Goubault13,GMP06}, used in
many companies, performs static analysis of \texttt{C} programs to prove
a bound on the rounding errors introduced by the use of
floating-point numbers instead of reals.  Fluctuat also helps users
debug floating-point errors by detecting the operations responsible for
significant precision loss.  Salsa~\cite{fmics15} automatically improves
the numerical accuracy of programs by using an abstract interpretation to
guide transformations that minimize the errors arising during
computations.  Herbie~\cite{pavel15} uses a heuristic search to improve the numerical
accuracy of an arithmetic expression by estimating and
localizing the roundoff errors of an expression using sampled points,
applying a set of rules in order to improve the accuracy of the expression
and combining these improvements for different input domains.
Rosa~\cite{DarulovaK14} combines
an exact SMT solver on reals with sound affine arithmetic to verify
accuracy post-conditions from assertions about the accuracy of inputs.
Rosa can guarantee that the desired precision
can be soundly obtained in a finite-precision implementation when
propagation error is included.  Finally,
FPTaylor~\cite{fptaylor-fm15} improves on interval arithmetic by using Taylor series to narrow
the computed error bounds.

As the number of tools dedicated to analyzing and improving numerical
accuracy grows, it becomes increasingly difficult to make fair
comparisons between the techniques.  This is because each tool is targeted
to slightly different domains, uses slightly different formats for
expressing benchmarks, and reports results using related but slightly
different measures.  Furthermore, without any standard set of floating-point
benchmarks, it is difficult to identify opportunities for composing
complementary tools.

To address these challenges, the floating-point research community needs a
standard benchmark format and common set of measures that enables
comparison and cooperation between tools.  This goal is motivated by the
success of standard benchmark suites like SPEC~\cite{spec} and
SPLASH-2~\cite{splash} in
the compiler community, the DIMACS~\cite{dimacs} format in the SAT-solving
community, and the SMT-LIB~\cite{smtlib} format in the SMT-solving community.
The formats have enabled fair comparisons between tools, crisp
characterizations of the tradeoffs between different approaches, and useful
cooperation between tools with complementary strengths.

In this article, we propose \name, a general floating-point format and
benchmark suite. \name provides a common language for floating-point tools,
allowing users to combine tools that perform complementary tasks,
or compare competing tools to choose the one best for their task.
The common scientific methodology \name enables
is crucial for demonstrating the
improvements of each tool on the state of the art.
In particular, \name enables tools which take in
FPBench formatted programs,
and produce measures or verifications of their accuracy,
as well as tools which take in such programs
and produce new, improved programs.

The main contributions of this article are the following:
\begin{enumerate}[label=(\roman*)]
%
\item A uniform format for expressing floating-point benchmarks, \core.
%
\item A set of utilities for converting to and from \core programs, and
  working with \core programs.
%
\item A set of measures on which to evaluate various floating-point tools
  on \name benchmarks.
%
\item An initial suite of benchmarks drawing from existing floating-point
  literature.
%
\end{enumerate}

The remainder of this article is organized as follows.
Section~\ref{sec:format}, describes the \name formats.
Section~\ref{sec:measure} introduces various measures of floating-point
error.  Section~\ref{sec:tools} describes the utilities \name provides to
support creating and working with benchmarks.
Section~\ref{sec:case-studies} surveys our existing benchmark suite,
highlighting representative case studies from recent tools in the
literature.  Finally, Section~\ref{sec:conclusion} discusses future work
and concludes.


%% In a world where trusting digital systems is increasingly important,
%% automatic program verification tools are mandatory to provide
%% certified results.  In this context floating-point computations are an
%% important issue because of the rounding errors done on each operation.
%% Indeed, floating-point arithmetic is not intuitive and very sensitive
%% to the roundoff errors.  To ensure the validity of the results
%% obtained with this arithmetic, several validating methods have been
%% introduced.  Among them, we may cite: Fluctuat~\cite{Goubault13,GMP06}
%% measures by static analysis an over-approximation of the error due to
%% the use of floating-point numbers instead of reals while executing a
%% program written in the \texttt{C} language.  It also, helps the user
%% to debug its code by detecting the responsible operations of the most
%% significant precision loss.  This approach is used by several
%% industries.  Salsa~\cite{fmics15} improves the numerical accuracy of
%% programs by automatic transformation.  It minimizes the value of the
%% errors arising during computations using error bounds obtained by
%% abstract interpretation.  Based on a search to improve the numerical
%% accuracy of an arithmetic expression, Herbie~\cite{pavel15} (Zachary
%% and al.) estimates and localizes the roundoff errors of an expression
%% using sampled points, applies a set of rules in order to improve the
%% accuracy of the expression and combines these improvements for the
%% different inputs.  Another approach introduced by~\cite{DarulovaK14}
%% and implemented in Rosa intends to combine an exact SMT solver based
%% on reals with an approximate and sound affine and interval arithmetic
%% computation.  Its use requires to set a desired post-condition and
%% involves the uncertainties as well as the desired target precision.
%% The compiler verifies that the desired precision can be soundly
%% obtained in finite-precision implementation while all the
%% uncertainties and their propagation are included.  Finally,
%% FP-Taylor~\cite{fptaylor-fm15} uses Taylor series developments to
%% narrow the error computed by the interval arithmetic.  This competes
%% with the affine arithmetic domain used by Fluctuat.
%% 
%% 
%% While there is a larger and larger number of tools dedicated to
%% improve the numerical accuracy of codes and to bound the errors
%% arising in floating-point computations, it becomes more and more
%% difficult to compare them on the same programs because of the absence
%% of a standard format and relevant suite for benchmarks and the fact
%% tools do not use the same measures of the accuracy. The current state
%% of floating point benchmarks also obscures opportunities for composing
%% tools that are complementary. With a common format, users could
%% compose a tool that does heuristic improvement with a tool that
%% provides sound error bounds.  Hence, the present work is motivated by
%% the absence of a global approach that provides us the opportunity to
%% compare the different tools previously mentioned, and use them in
%% tandem. In addition, having a common scientific methodology is very
%% important to show the improvements of each tool.
%% 
%% 
%% The main contributions of this article are the following:
%% \begin{enumerate}[label=(\roman*)]
%% \item A uniform format for expressing floating point benchmarks, \core.
%% \item A set of tools for converting to and from \core programs, and
%%   working with \core programs.
%% \item A set of measures on which to evaluate various tools on \name
%%   benchmarks.
%% \item An initial suite of benchmarks drawing from existing floating
%%   point literature.
%% \end{enumerate}
%% 
%% 
%% This article is outlined as following. In Section~\ref{sec:format}, we
%% recall some basic notions on term representing as well as the
%% benchmark format. In Section~\ref{sec:measure} we introduce the
%% different kinds of measures of the error. Section~\ref{sec:tools}
%% describes a set of tools released with FPBench to make creating and
%% working with benchmarks easier. We discuss in
%% Section~\ref{sec:case-studies} our existing benchmark suite,
%% highlighting a few representative case studies. Finally, in
%% Section~\ref{sec:conclusion}, conclusions are drawn and further work
%% is suggested.




%There is an explosion of floating point tools, including
%FPTaylor~\cite{fptaylor-fm15}.

%In summary, this paper contributes:
%
%\begin{enumerate}
%
%  \item An expressive format for floating point point benchmarks.
%
%  \item A set of measures for evaluating performance of various tools on  these benchmarks.
%
% \item Examples taken from existing tools in the literature an shown to be   expressible in FPBench.
%
%\end{enumerate}

\end{document}
