\documentclass[main.tex]{subfiles}
\begin{document}

\section{Introduction}
\label{sec:intro}

Developing reliable software is increasingly crucial, and many recent
advances in automatic program analysis and verification can now provide
strong guarantees for real industrial code~\todo{cites}.  However these
techniques are typically focused on integer programs, and do not apply to
the floating-point computations we depend on for safety-critical control in
embedded systems or avionics nor the analyses carried out by scientific and
computer-aided design applications.  In these contexts, floating-point
computations are a critical issue due to rounding errors that arise on
every operation.  Indeed, floating-point arithmetic is notoriously
unintuitive and its sensitivity to roundoff errors makes such computation
fiendishly difficult to debug.  Traditionally, such errors have been
addressed by numerical methods experts who manually analyze and rewrite
floating-point code to ensure accuracy and stability.  However, these
manual techniques are difficult to apply and typically do not lead to a
independently checkable certificate assuring the correctness of the
results.

The research community has responded to these challenges by introducing
several diverse automated techniques that provide guaranteed bounds on the
accuracy of floating-point computations or attempt to automatically improve
accuracy.  For example, Fluctuat~\cite{Goubault13,GMP06}, which is used in
many companies, performs static analysis of \texttt{C} program to provide
an over-approximation of the rounding error introduced by computing with
floating-point numbers instead of reals.  Fluctuat also helps the user
debug their code by detecting the operations responsible for the most
significant precision loss.  Salsa~\cite{fmics15} automatically improves
the numerical accuracy of programs by using an abstract interpretation to
guide transformations that minimize the value of the errors arising during
computations.  Based on a search to improve the numerical accuracy of an
arithmetic expression, Herbie~\cite{pavel15} estimates and localizes the
roundoff errors of an expression using sampled points, applies a set of
rules in order to improve the accuracy of the expression and combines these
improvements for the different inputs.  Another approach introduced
by~\cite{DarulovaK14} and implemented in Rosa intends to combine an exact
SMT solver based on reals with an approximate and sound affine and interval
arithmetic computation.  Its use requires to set a desired post-condition
and involves the uncertainties as well as the desired target precision.
The compiler verifies that the desired precision can be soundly obtained in
finite-precision implementation while all the uncertainties and their
propagation are included.  Finally, FP-Taylor~\cite{fptaylor-fm15} uses
Taylor series developments to narrow the error computed by the interval
arithmetic.  This competes with the affine arithmetic domain used by
Fluctuat.

As the number of tools dedicated to analyzing and improving numerical
accuracy continues growing, it becomes increasingly difficult to make fair
comparisons between the techniques.  This is because each tool is targeted
to slightly different domains, uses slightly different formats for
expressing benchmarks, and reports results using related but slightly
different measures.  Furthermore, without any standard set of floating
point benchmarks, it is difficult to identify opportunities for composing
complementary tools.

In this article, we propose FPBench, a common floating-point benchmark
format. FPBench enables users to compose tools that perform heuristic
improvements with complementary tools that provide sound error bounds.
Hence, FPBench provides a common language that can be used to develop a
global approach for comparing the various tools previously mentioned, and
use them in tandem. In addition, having a common scientific methodology is
crucial to show the improvements of each tool on the state of the art.


The main contributions of this article are the following:
\begin{enumerate}[label=(\roman*)]
\item A uniform format for expressing floating point benchmarks, \core.
\item A set of tools for converting to and from \core programs, and
  working with \core programs.
\item A set of measures on which to evaluate various tools on \name
  benchmarks.
\item An initial suite of benchmarks drawing from existing floating
  point literature.
\end{enumerate}

The remainder of the article is organized as follows.  In
Section~\ref{sec:format}, we recall some basic notions on term representing
as well as the benchmark format. In Section~\ref{sec:measure} we introduce
the different kinds of measures of the error. Section~\ref{sec:tools}
describes a set of tools released with FPBench to make creating and working
with benchmarks easier. We discuss in Section~\ref{sec:case-studies} our
existing benchmark suite, highlighting a few representative case studies.
Finally, in Section~\ref{sec:conclusion}, conclusions are drawn and further
work is suggested.



%% In a world where trusting digital systems is increasingly important,
%% automatic program verification tools are mandatory to provide
%% certified results.  In this context floating-point computations are an
%% important issue because of the rounding errors done on each operation.
%% Indeed, floating-point arithmetic is not intuitive and very sensitive
%% to the roundoff errors.  To ensure the validity of the results
%% obtained with this arithmetic, several validating methods have been
%% introduced.  Among them, we may cite: Fluctuat~\cite{Goubault13,GMP06}
%% measures by static analysis an over-approximation of the error due to
%% the use of floating-point numbers instead of reals while executing a
%% program written in the \texttt{C} language.  It also, helps the user
%% to debug its code by detecting the responsible operations of the most
%% significant precision loss.  This approach is used by several
%% industries.  Salsa~\cite{fmics15} improves the numerical accuracy of
%% programs by automatic transformation.  It minimizes the value of the
%% errors arising during computations using error bounds obtained by
%% abstract interpretation.  Based on a search to improve the numerical
%% accuracy of an arithmetic expression, Herbie~\cite{pavel15} (Zachary
%% and al.) estimates and localizes the roundoff errors of an expression
%% using sampled points, applies a set of rules in order to improve the
%% accuracy of the expression and combines these improvements for the
%% different inputs.  Another approach introduced by~\cite{DarulovaK14}
%% and implemented in Rosa intends to combine an exact SMT solver based
%% on reals with an approximate and sound affine and interval arithmetic
%% computation.  Its use requires to set a desired post-condition and
%% involves the uncertainties as well as the desired target precision.
%% The compiler verifies that the desired precision can be soundly
%% obtained in finite-precision implementation while all the
%% uncertainties and their propagation are included.  Finally,
%% FP-Taylor~\cite{fptaylor-fm15} uses Taylor series developments to
%% narrow the error computed by the interval arithmetic.  This competes
%% with the affine arithmetic domain used by Fluctuat.
%% 
%% 
%% While there is a larger and larger number of tools dedicated to
%% improve the numerical accuracy of codes and to bound the errors
%% arising in floating-point computations, it becomes more and more
%% difficult to compare them on the same programs because of the absence
%% of a standard format and relevant suite for benchmarks and the fact
%% tools do not use the same measures of the accuracy. The current state
%% of floating point benchmarks also obscures opportunities for composing
%% tools that are complementary. With a common format, users could
%% compose a tool that does heuristic improvement with a tool that
%% provides sound error bounds.  Hence, the present work is motivated by
%% the absence of a global approach that provides us the opportunity to
%% compare the different tools previously mentioned, and use them in
%% tandem. In addition, having a common scientific methodology is very
%% important to show the improvements of each tool.
%% 
%% 
%% The main contributions of this article are the following:
%% \begin{enumerate}[label=(\roman*)]
%% \item A uniform format for expressing floating point benchmarks, \core.
%% \item A set of tools for converting to and from \core programs, and
%%   working with \core programs.
%% \item A set of measures on which to evaluate various tools on \name
%%   benchmarks.
%% \item An initial suite of benchmarks drawing from existing floating
%%   point literature.
%% \end{enumerate}
%% 
%% 
%% This article is outlined as following. In Section~\ref{sec:format}, we
%% recall some basic notions on term representing as well as the
%% benchmark format. In Section~\ref{sec:measure} we introduce the
%% different kinds of measures of the error. Section~\ref{sec:tools}
%% describes a set of tools released with FPBench to make creating and
%% working with benchmarks easier. We discuss in
%% Section~\ref{sec:case-studies} our existing benchmark suite,
%% highlighting a few representative case studies. Finally, in
%% Section~\ref{sec:conclusion}, conclusions are drawn and further work
%% is suggested.




%There is an explosion of floating point tools, including
%FPTaylor~\cite{fptaylor-fm15}.

%In summary, this paper contributes:
%
%\begin{enumerate}
%
%  \item An expressive format for floating point point benchmarks.
%
%  \item A set of measures for evaluating performance of various tools on  these benchmarks.
%
% \item Examples taken from existing tools in the literature an shown to be   expressible in FPBench.
%
%\end{enumerate}

\end{document}
